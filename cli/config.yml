path: ..

logLevel: info # error| info | warn | error | debug | silly

# The default provider to use for all commands
llmProvider: ollama # openai | ollama

tools:
  - name: codeSummaryQuery
    module: '@interrobangc/codellm-tool-code-summary-query'
  - name: docSummaryQuery
    module: '@interrobangc/codellm-tool-doc-summary-query'
  - name: jsDependencyTree
    module: '@interrobangc/codellm-tool-js-dependency-tree'

# The configuration for individual providers can be set here. You probably want to use
# the OPEN_AI_API_KEY or MISTRAL_API_KEY environment variable instead of hardcoding your key here
providers:
  mistral:
    config:
      apiKey: ''

  openai:
    config:
      apiKey: ''

  ollama:
    config:
      host: 'http://localhost:11434'
## Individual providers and models for various parts of the system can be configured here
## Available Ollama models can be found here: https://ollama.com/library/
## Available Openai models can be found here: https://platform.openai.com/account/limits
# llms:
#   agent:
#     provider: ollama
#     model: mixtral:8x7b
#   embedding:
#     provider: ollama
#     model: nomic-embed-text
#   summarize:
#     provider: ollama
#     model: mixtral:8x7b
#   tool:
#     provider: ollama
#     model: mixtral:8x7b

