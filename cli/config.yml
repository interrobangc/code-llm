project:
  name: 'codellm'

paths:
  project: ..
  cache: ../.cache

logLevel: info # error| info | warn | error | debug | silly

# The default provider to use for all commands
llmProvider: ollama # openai | ollama

# The format to use for the output of LLM messages so that the agent can understand it
# is sent to the LLM in the initial system message and all subsequent user messages
# when formatInUserMessage is true. This helps less powerful LLMs to "remember" how to
# format the output for the user. If you are using a more powerful LLM and paying for
# message tokens, you can set this to false and the agent will only send the format to
# the LLM in the initial system message which will save you tokens and processing time.
formatInUserMessage: true

tools:
  codeElementsQuery:
    module: '@codellm/tool-code-elements-query'
    config:
      vectorDbName: chromadb
      vectorDbCollectionName: codeElements
  codeSummaryQuery:
    module: '@codellm/tool-code-summary-query'
    config:
      vectorDbName: chromadb
      vectorDbCollectionName: codeSummary
  # docSummaryQuery:
  #   module: '@codellm/tool-doc-summary-query'
  #   config:
  #     vectorDbName: chromadb
  #     vectorDbCollectionName: docSummary
  fileReader:
    module: '@codellm/tool-file-reader'
    config:
      maxFileCount: 8
  # jsDependencyTree:
  #   module: '@codellm/tool-js-dependency-tree'
  projectGlob:
    module: '@codellm/tool-project-glob'
